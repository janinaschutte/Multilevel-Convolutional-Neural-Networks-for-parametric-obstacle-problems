import numpy as np
import torch
from torch.optim import Adam
from torch.optim.lr_scheduler import LambdaLR
from dolfin import *
import time
from tqdm import tqdm
import os
import pickle

from .Model import BigConvModel, BigVCycleModel
from .DataHandler import MultiLevelRefinementSampler, load_problem_info_dict
from .TrainModel import get_transforms_and_orderings, LossFunct, build_pt_obstacle_dataset
from .BuildDataSets import build_problem
uniform100_y = np.array([0.4745084677788005, 0.732686499397869, 0.2846447257460598, 0.294701332952531, 0.7623223941852393, -0.3652941107755876, 0.7930398809366843, 0.29760595937083667, 0.8668877507062653, -0.959706334032175, -0.2994223574346633, -0.36530928579597566, 0.31674445366270176, -0.05661444787938397, -0.8602878952443358, -0.7830325351653145, 0.4694035462584729, 0.41984108232913053, -0.9222678880419897, -0.3827079456848721, -0.9673272108460698, -0.15612907945571597, 0.15620035971942903, -0.5331711626994831, -0.715127859932781, -0.24456970588062288, -0.5424229364197293, -0.4794757975081818, 0.18991110212664197, 0.5981716756083848, 0.7620240578461599, 0.6398382516829446, 0.09817500921841149, 0.9260862493832953, 0.0337264788531193, -0.23002136741926482, -0.9948354210797288, -0.5538354152833656, 0.2510936444917711, 0.6700593398276267, 0.635038324065021, 0.9969447610193451, -0.3074247513727584, -0.3508241921907487, 0.0172698598718668, 0.8596540870859293, 0.329058672062265, -0.21433664411180464, -0.2860102726200977, -0.674626612917117, 0.37691080308737246, 0.19741137913136964, 0.3875674854435509, 0.6395614485440932, 0.1603054128789534, -0.9677274409658625, -0.5562302905710323, 0.06097890781349946, -0.596972694824889, 0.03864468445319513, -0.8634841937192694, 0.625426015207952, 0.4600872923319428, -0.5142427301284356, 0.9953636304380558, 0.48103303551247434, 0.2942701523450697, -0.0424614632734448, -0.5379628022403755, -0.7584861540426189, 0.1789165435890152, -0.5964743983245309, -0.2547435808197829, 0.9679118352401528, 0.7263753063381919, -0.6963372836334105, 0.9283918318822526, 0.19835917569952466, -0.5132103878268939, -0.9643816084165107, -0.7380688239285877, 0.4114445812914178, -0.7190994765098435, -0.768141079601403, 0.21817101370788428, 0.5257096019297989, 0.32611205724307224, 0.3893496033523385, -0.25261793249336617, -0.824793242279505, -0.3853364530532062, -0.832061426670488, -0.3180527615109263, 0.07558982002373194, -0.8538539124607523, 0.7226156431436173, 0.8930270044698647, 0.022267055614106246, -0.21126275866887978, 0.7722298299697867])
uniform10_y = np.array([-0.6153438848932424, 0.3549405886606667, -0.281002665309233, -0.3370854742672087, 0.5104213470050483, 0.3489914139541468, -0.993005557089528, 0.16971500951669127, 0.9889074777993239, 0.3597414933853502])
lognormal100_y = np.array([0.046118962589851445, 0.8738893417178639, 1.94566379179238, -1.2220885877038978, -0.07731109741186144, 0.7190998582914286, -0.0808427903178289, -0.28285975712427086, 0.003692768702777094, 0.9139271395027802, -0.6311842579990051, 0.6496432282522447, 0.14787782707890731, 0.8698498233201317, -1.422178136832103, -2.0171502787142517, 1.2352393124851457, -1.16339991304478, -0.4901522772572464, 1.1133132506830523, 0.02054223558348826, 0.24130380857669192, -1.5418931658362596, -0.5666937214510426, -0.09853902954259154, 0.4983818892580964, -0.7947083247925362, 0.7999930431655217, 0.012644337241996415, 0.24714870547910187, 0.7177209208205928, 1.1203263199228266, -0.012495214997876165, 1.3661820750191884, -1.650814245836327, 0.17381400007489808, -0.900057005061954, 0.06772684733953456, 1.6537693187166402, 0.6643475759170707, 0.43972887134849176, -2.158850945531405, 1.1192986954492725, -0.5909529184757255, 0.02506323959823711, -0.2314735829985688, -0.8580400945643989, 0.32836957517982795, -0.7025935154298, -2.804835936829763, -1.015798012664836, -0.029532072783445756, 0.40437362739626975, -1.2547096890219578, 0.019014688381111176, -0.3987067780622173, -0.13746193367815393, 0.14708411001397204, 0.8626522918754312, 0.3881069102667288, 1.970080066516573, -1.7253784261007095, -1.9870799858152106, -0.9828215321997225, 0.09350780071000908, 0.8892106419036179, 0.19151639896499806, -0.3054239323471711, -0.5577774085283017, 0.20801167759870107, -2.2559937096292697, 0.29533732200446405, 0.08057285304246521, -1.0461516712358736, -1.105191589187244, 0.29100251286946843, -0.49210778849242154, 0.1764926252195891, 0.8441170751530855, -0.40309670026154976, -0.4418919795706322, 0.28399076846333093, 0.8139378635570989, -0.6351828317207183, 0.9290008168896564, 0.5775997148093063, 1.1060064649613455, 0.40763896635752367, -0.05222176982453041, 0.25244252818012164, 0.8685226355293373, 0.4154428344845119, -1.8303698825108758, 1.9534872717529388, 0.4354240137112141, 0.9176894834596435, -0.02180219442931733, 0.539114886347816, -0.05987463878697069, -0.23025356037855])
cookie16_y = np.array([0.8652747929915803, 0.5162419524475841, 0.14216054902718445, 0.5339616670173304, 0.23172494159663337, -0.6691350407498586, 0.6887055658683168, -0.881335060341909, 0.6668792647016524, -0.6044867298376273, -0.955235799957916, 0.9425117207626166, 0.3458377121107179, -0.8709850084064206, 0.09489966959551688, 0.7393272984631356])
cookie64_y = np.array([0.08912267970929455, 0.08484278770910669, -0.1979187642878284, -0.7570197226563222, 0.11808083214285592, 0.6614431293051009, 0.6657353792661875, -0.6794732648810502, -0.005769492595479653, 0.25743605563630045, 0.442350590460743, -0.6096494745714816, 0.7182663675593148, 0.24112501636280004, -0.7029229613165677, 0.4546856944494264, 0.59535749619375, 0.2996988827623228, -0.5287537812628476, 0.045125751709812256, 0.9486670270708233, -0.11898008974577512, 0.8661531856634062, -0.3221453385915487, -0.6742015991408579, 0.7699649011269631, -0.2804103528373947, -0.7163005174241208, -0.1781935172175182, -0.2536277938922207, -0.5003558425765713, 0.48524308881155176, 0.6220173561145155, -0.44613991321832835, 0.8526216667674573, 0.21376710444574631, -0.31890330130888445, 0.7897103751405175, 0.8569213636454696, 0.7437078884999695, 0.7871091642301271, 0.36922510255266827, 0.739709431441981, 0.5769052325558932, -0.051773965408132616, 0.6618819411507875, 0.07308742900543885, 0.31525338354477483, 0.1741352705800332, 0.9816200118861089, 0.22716177436333385, -0.6372198976589252, -0.6279844311783567, -0.1520740616156009, -0.5969216615287787, -0.5779157729071209, -0.7345402685211042, 0.20016167373473714, 0.8830780470922952, 0.8836178354299218, -0.16184634320009117, 0.14771665501858955, 0.1867017272070457, 0.21238796374748325])
cookie32_y = np.array([0.7127885944683887, 0.3288299717035459, -0.7234491992042698, -0.4879502800196054, 0.3189511804844678, 0.5051886561767198, -0.7854538381746234, -0.8597775579023919, -0.7497935938467648, 0.7840122811301129, 0.14310018150218795, -0.7795579312577403, 0.3227942234407404, 0.013008948867442527, -0.7965977781015785, 0.737170062091401, 0.9859955773299591, 0.03212992423787853, -0.30068346683898395, 0.3905296090041026, 0.027615163326751846, -0.5436649992913754, 0.5572752855036465, -0.04438054441642958, -0.8670889295786601, 0.8882474628149895, -0.7173565287520283, -0.9149868356186055, 0.5510876010983012, -0.6622520896980673, -0.10901922385226714, -0.029504665400253982])
cookie128_y = np.array([0.14073334768041756, 0.24502094104298244, 0.1448084098190221, 0.6847535184177092, -0.36255401341826077, 0.2822977370277089, -0.8201011013270016, -0.6675359321946155, 0.8469500799606304, -0.7021351251003605, 0.7280530982076019, 0.7684163279712009, 0.5721118030944252, 0.40502518196954207, 0.6625158004690941, -0.7900614315274268, -0.14405348528111794, 0.9221656994731477, -0.9729110997775237, 0.5314395939927741, 0.2365348588070273, 0.8228646658032392, -0.7172853483471475, -0.6507870230505823, 0.017701625454188408, -0.02243247044793839, -0.476134036958094, -0.8296360781603223, 0.37385517229474985, 0.37280508901961684, -0.9770327888982675, 0.24509145339966643, 0.4450068689725144, -0.4084766203647898, 0.983418059492343, 0.10910228376626163, 0.7862464556425715, -0.07548749169302016, -0.6188284963699531, 0.1310153777450953, 0.48292156146380005, 0.6982630343590299, 0.764015683117075, 0.04309579266679919, 0.3208224728481126, 0.6048533191561547, -0.33838972425733105, 0.7935956934994937, -0.23666785535372004, 0.21171031083938585, 0.8073587986507773, -0.41727502306643904, 0.5583234604509351, -0.6242993096190197, -0.693114169418777, -0.29270962362644504, 0.36622297410027715, 0.20750365572268525, 0.9796729065246208, 0.4932590200865996, -0.8129072630067062, 0.6229335502557309, -0.18628213880477595, 0.7587828251502531, 0.14842173752276477, -0.12420971344196574, 0.4501963491229952, -0.21825988103278293, -0.29865192046215516, 0.08768976329894307, 0.03380060219811343, 0.4378355527810476, 0.18868845778099597, 0.9540614611939136, -0.541559894500292, -0.3546699867829537, -0.9875526459909749, -0.5259898134061698, 0.22180579558770153, -0.23133119890446596, -0.49885330424079277, 0.15553675324075678, 0.14488980868223345, 0.8718497083932706, 0.7486056052657821, -0.6996294779760728, -0.24319716339087005, 0.19951107148189062, -0.4484030953355984, -0.7009712754489823, -0.9272172909160201, 0.07386899242990608, -0.3897192623046819, 0.5205506309198051, -0.9595310457701416, 0.3341286178797349, -0.15033029481043325, 0.17462288931470638, -0.5708757381406335, 0.691039770513618, -0.6409656734165414, -0.36755874491994334, -0.3774919790106257, -0.488115104911919, -0.010678342079745606, -0.11658830145581756, 0.4945936124914829, 0.8076914179242907, 0.2763433916534228, 0.7563356810450175, 0.46421936874939695, -0.7843043461941748, -0.5323952931373339, 0.6671491985075799, -0.26402734038955544, -0.6171874623764573, 0.91421488433723, -0.8856509704129416, -0.8036316749979961, -0.32707780663424946, 0.18494744055142043, 0.17001614965660905, 0.9031347342101508, -0.5840808418087418, -0.39270374095596705, -0.4716924590755873, 0.17864236003596945, -0.7215923833443749])
#obstacle50_y = np.array([-0.576280168670769, 0.3163661728833058, 0.2756439949282188, 0.09865664572335753, -0.19511538521070304, 0.5695028864621647, 0.017436647961814877, -0.653424610918685, -0.936045810827993, -0.9559142283073512, 0.9042432378408638, -0.5370812892806265, 0.05377614238174022, 0.6545856903455123, 0.3887037384426646, 0.30356161762685585, 0.17882026850295518, -0.7700379449648558, 0.0071972801737425485, -0.36896552095472845, 0.7839935014667134, 0.021454942888592177, -0.9570041352970107, 0.20429116357925214, -0.006660459244693495, -0.28602128505208246, -0.6965239126090939, 0.6557572479744607, 0.32245771929006417, 0.8635623352702975, 0.0741080724087575, 0.5537744819416188, 0.10314018498888711, 0.014816928404640661, -0.16410304682198373, 0.4076903054408172, 0.038338543198742414, 0.7894861035431084, 0.8889906957053992, -0.21717586293025604, -0.09297220440751808, -0.8694512911633618, 0.7722350346708293, 0.035411153721787825, -0.2227408615256845, -0.8433923123542779, -0.27587337699444725, -0.005680593590549066, -0.9429975444310854, 0.16829939484810286])
#obstacle51_y = np.array([-0.8601224985532756, -0.7534669420981788, -0.2238871913081475, 0.814060690100308, -0.4696093682371396, 0.023633138316808022, 0.8299547088050905, -0.8234476445474939, 0.6451957990471708, 0.8548360157614008, 0.9203188909326221, 0.0894937741781412, 0.5529481582601514, -0.1713154325167383, 0.02076590288419422, -0.9377842797184399, -0.4593464412391135, 0.9221072920159221, -0.7434194829658318, 0.5206868403670362, 0.8587189479406525, -0.6814661576856937, -0.4349162079068287, 0.5929900535024413, 0.626401735750195, 0.03473497125499625, 0.12186539149795039, -0.20022998816043902, 0.35444397370359093, -0.7727801911443644, 0.9921816572389854, -0.45972224788699245, -0.9184230877849713, 0.3550886702911267, -0.2893289320472283, -0.4988786141542889, -0.8927652840153049, -0.5126056038118465, 0.5701170587544238, -0.1438298672284779, -0.47646552881151893, 0.3216584575237844, -0.8852552387295942, -0.11514990315486706, -0.001442233886143729, -0.29366302238148245, 0.813847257947582, -0.22502442991228522, 0.192575790231986, -0.8324237016103908, -0.039236997059255097])

def show_example_image(metrics_dict_path, saving_path_extra, param_vector, device):
    metrics_dict_path = './code/FinalConvModel/ModelSaves/' + metrics_dict_path
    metrics_dict = np.load(metrics_dict_path, allow_pickle=True)[None][0]
    input_params = metrics_dict['input_params']
    data_loading_path = input_params['data_loading_path']
    num_test_samples = input_params['num_test_samples']
    num_corrections = input_params['num_corrections']
    y_img_mean = metrics_dict['y_img_mean']
    y_img_std = metrics_dict['y_img_std']
    obstacle_parameter = input_params['obstacle_parameter']
    saving_path = input_params['saving_path']
    run_name = input_params['run_name']
    target_stds = metrics_dict['target_stds']
    target_means = [x.to(device) for x in metrics_dict['target_means']]
    model_size_per_level = input_params['model_size_per_level']
    batch_size = input_params['batch_size']

    #DataSampler = pickle.load(open(os.path.join(data_loading_path, 'data_sampler.p'), 'rb'))
    #DataSampler.reinit_sampler()
    problem_info_dict = build_problem(input_params['problem'], input_params['param_dimension'],
                                      num_corrections, obstacle_parameter, input_params['fixed_radii'])
    DataSampler = MultiLevelRefinementSampler(problemInfo=problem_info_dict, overwrite_levels=num_corrections)

    y_imgs = DataSampler.get_ys_images([param_vector], num_corrections - 1)
    u_imgs = DataSampler.get_solutions_images_from_ys([param_vector], num_corrections - 1)
    n, m = u_imgs[0].shape
    loss_function = LossFunct(DataSampler, num_corrections, device, None, norm='h1', small_mode=True)

    model = BigConvModel(num_corrections, model_size_per_level, obstacle_parameter,
                         *get_transforms_and_orderings(DataSampler, num_corrections, device))
    model.to(device)
    model.load_state_dict(torch.load(os.path.join(saving_path, 'model_{}.pt'.format(run_name))))
    model.eval()

    y_img_pt = torch.as_tensor(y_imgs).to(torch.float32).to(device)[:, None, :, :]
    y_img_pt = (y_img_pt - y_img_mean) / y_img_std
    if obstacle_parameter:
        obstacle_img = DataSampler.get_ys_obstacle_images(np.array([param_vector]), num_corrections - 1)
        input_batch_obstacle = build_pt_obstacle_dataset(DataSampler, obstacle_img, num_corrections, device)
        input_batch_obstacle = [x.to(device) for x in input_batch_obstacle]
        input_batch = input_batch_obstacle[:-1] + [y_img_pt]
    else:
        input_batch_obstacle = None
        input_batch = [torch.zeros((1, 1, 1, 1), device=device) for i in range(num_corrections - 1)] + [y_img_pt]

    with torch.no_grad():
        t1 = time.perf_counter()
        model_out = model(input_batch, obstacle_levels=input_batch_obstacle, num_levels=num_corrections)
        t2 = time.perf_counter()
        print(t2-t1)
        outs = [x.detach() for x in model_out]
        outs = [x * std + target_mean for x, std, target_mean in zip(outs, target_stds, target_means)]

    summed_outs = [outs[0]]
    for i in range(1, num_corrections):
        summed_outs.append(loss_function.upsample(summed_outs[-1], i - 1) + outs[i])

    final_out = summed_outs[-1].cpu().numpy().astype(np.float64)[:, 0, :, :]

    data = np.concatenate((y_imgs, u_imgs, final_out), axis=0)
    np.save(saving_path_extra, data)
    print('done with', saving_path_extra)


def show_example_image_vnet(metrics_dict_path, saving_path_extra, param_vector, device):
    metrics_dict_path = './code/FinalConvModel/ModelSaves/' + metrics_dict_path
    metrics_dict = np.load(metrics_dict_path, allow_pickle=True)[None][0]
    input_params = metrics_dict['input_params']
    data_loading_path = input_params['data_loading_path']
    num_test_samples = input_params['num_test_samples']
    num_corrections = input_params['num_corrections']
    y_img_mean = metrics_dict['y_img_mean']
    y_img_std = metrics_dict['y_img_std']
    obstacle_parameter = input_params['obstacle_parameter']
    saving_path = input_params['saving_path']
    run_name = input_params['run_name']
    target_std = metrics_dict['target_std']
    target_mean = metrics_dict['target_mean']
    model_size = input_params['model_size']
    batch_size = input_params['batch_size']

    #DataSampler = pickle.load(open(os.path.join(data_loading_path, 'data_sampler.p'), 'rb'))
    #DataSampler.reinit_sampler()
    problem_info_dict = build_problem(input_params['problem'], input_params['param_dimension'],
                                      num_corrections, obstacle_parameter, input_params['fixed_radii'])
    DataSampler = MultiLevelRefinementSampler(problemInfo=problem_info_dict, overwrite_levels=num_corrections)

    y_imgs = DataSampler.get_ys_images([param_vector], num_corrections - 1)
    u_imgs = DataSampler.get_solutions_images_from_ys([param_vector], num_corrections - 1)
    n, m = u_imgs[0].shape
    loss_function = LossFunct(DataSampler, num_corrections, device, None, norm='h1', small_mode=True)

    model = BigVCycleModel(num_corrections, model_size, obstacle_parameter)
    model.to(device)
    model.load_state_dict(torch.load(os.path.join(saving_path, 'model_{}.pt'.format(run_name))))
    model.eval()

    y_img_pt = torch.as_tensor(y_imgs).to(torch.float32).to(device)[:, None, :, :]
    y_img_pt = (y_img_pt - y_img_mean) / y_img_std
    if obstacle_parameter:
        raise NotImplementedError()
    else:
        input_batch_obstacle = None
        input_batch = y_img_pt

    with torch.no_grad():
        t1 = time.perf_counter()
        model_out = model(input_batch, obstacle_levels=input_batch_obstacle)
        t2 = time.perf_counter()
        print(t2-t1)
        out_sample = model_out.detach()
        out_sample = out_sample * target_std + target_mean.to(device)

    final_out = out_sample.cpu().numpy().astype(np.float64)[:, 0, :, :]

    data = np.concatenate((y_imgs, u_imgs, final_out), axis=0)
    np.save(saving_path_extra, data)
    print('done with', saving_path_extra)



if __name__ == "__main__":
    show_example_image('TrainingMetricsDict_uniform100level7.npy', './code/MakeFigures/uniform100_fnet_sols.npy',
                       uniform100_y, torch.device('cuda:0'))

    show_example_image_vnet('TrainingMetricsDict_uniform10OnlyVcyclesvcycles.npy',
                            './code/MakeFigures/uniform10_vnet_sols.npy', uniform100_y, torch.device('cuda:0'))

    #show_example_image('TrainingMetricsDict_uniform10level7.npy', './code/MakeFigures/uniform10_fnet_sols.npy', uniform10_y, torch.device('cuda:0'))
    #show_example_image('TrainingMetricsDict_lognormal100level7.npy', './code/MakeFigures/lognormal100_fnet_sols.npy', lognormal100_y, torch.device('cuda:0'))
    #show_example_image('TrainingMetricsDict_cookie16fixedlevel7.npy', './code/MakeFigures/cookie16_fnet_sols.npy', cookie16_y, torch.device('cuda:0'))
    #show_example_image('TrainingMetricsDict_cookie64fixedlevel7.npy', './code/MakeFigures/cookie64_fnet_sols.npy', cookie64_y, torch.device('cuda:0'))
    #show_example_image('TrainingMetricsDict_cookie32varlevel7.npy', './code/MakeFigures/cookie32_fnet_sols.npy', cookie32_y, torch.device('cuda:0'))
    #show_example_image('TrainingMetricsDict_cookie128varlevel7.npy', './code/MakeFigures/cookie128_fnet_sols.npy', cookie128_y, torch.device('cuda:0'))
    #show_example_image('TrainingMetricsDict_obstacle50level7.npy', './code/MakeFigures/obstacle50_fnet_sols.npy', obstacle50_y, torch.device('cuda:0'))
    #show_example_image('TrainingMetricsDict_obstacle51level7.npy', './code/MakeFigures/obstacle51_fnet_sols.npy', obstacle51_y, torch.device('cuda:0'))

    #show_example_image_vnet('TrainingMetricsDict_uniform10OnlyVcyclesvcycles.npy', './code/MakeFigures/uniform10_vnet_sols.npy', uniform10_y, torch.device('cuda:0'))
    #show_example_image_vnet('TrainingMetricsDict_lognormal100OnlyVcycles_extra1vcycles.npy', './code/MakeFigures/lognormal100_vnet_sols.npy', lognormal100_y, torch.device('cuda:0'))
    #show_example_image_vnet('TrainingMetricsDict_cookie16OnlyVcyclesvcycles.npy', './code/MakeFigures/cookie16_vnet_sols.npy', cookie16_y, torch.device('cuda:0'))
    #show_example_image_vnet('TrainingMetricsDict_cookie64OnlyVcyclesvcycles.npy', './code/MakeFigures/cookie64_vnet_sols.npy', cookie64_y, torch.device('cuda:0'))
    #show_example_image_vnet('TrainingMetricsDict_cookie32OnlyVcyclesvcycles.npy', './code/MakeFigures/cookie32_vnet_sols.npy', cookie32_y, torch.device('cuda:0'))
    #show_example_image_vnet('TrainingMetricsDict_cookie128OnlyVcyclesvcycles.npy', './code/MakeFigures/cookie128_vnet_sols.npy', cookie128_y, torch.device('cuda:0'))
